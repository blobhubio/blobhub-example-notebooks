{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ONNX Tutorial - Super Resolution Vision Model - End to End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tutorial demonstrates complete lifecycle of neural model including  \n",
    "- training (or rather exporting pretrained weights)\n",
    "- storage (this is where BlobHub comes into play)\n",
    "- and subsequent inference.\n",
    "\n",
    "The model used in this tutorial is trained to scale up images (increase image resolution).\n",
    "\n",
    "References: \n",
    "- Original tutorial  \n",
    "  https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [Prerequisites](#Prerequisites)\n",
    "- [Install Dependencies](#Install-Dependencies) \n",
    "- [Export the Model](#Export-the-Model)  \n",
    "- [Upload Model to BlobHub](#Upload-Model-to-BlobHub)  \n",
    "- [Download Model from BlobHub](#Download-Model-from-BlobHub)  \n",
    "- [Run Model Inference](#Run-Model-Inference)  \n",
    "- [Compare Results](#Compare-Results)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is required to create BlobHub blob and API key (with `write` access level) in order to execute the tutorial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORG_ID = \"<< Org ID >>\"\n",
    "BLOB_ID = \"<< Blob ID >>\"\n",
    "API_KEY = \"<< API Key >>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install blobhub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define model topology:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "\n",
    "class SuperResolutionNet(nn.Module):\n",
    "    def __init__(self, upscale_factor, inplace=False):\n",
    "        super(SuperResolutionNet, self).__init__()\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=inplace)\n",
    "        self.conv1 = nn.Conv2d(1, 64, (5, 5), (1, 1), (2, 2))\n",
    "        self.conv2 = nn.Conv2d(64, 64, (3, 3), (1, 1), (1, 1))\n",
    "        self.conv3 = nn.Conv2d(64, 32, (3, 3), (1, 1), (1, 1))\n",
    "        self.conv4 = nn.Conv2d(32, upscale_factor ** 2, (3, 3), (1, 1), (1, 1))\n",
    "        self.pixel_shuffle = nn.PixelShuffle(upscale_factor)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.pixel_shuffle(self.conv4(x))\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        init.orthogonal_(self.conv1.weight, init.calculate_gain('relu'))\n",
    "        init.orthogonal_(self.conv2.weight, init.calculate_gain('relu'))\n",
    "        init.orthogonal_(self.conv3.weight, init.calculate_gain('relu'))\n",
    "        init.orthogonal_(self.conv4.weight)\n",
    "\n",
    "# Create the super-resolution model by using the above model definition.\n",
    "torch_model = SuperResolutionNet(upscale_factor=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load previously trained model parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "\n",
    "# Load pretrained model weights\n",
    "model_url = 'https://s3.amazonaws.com/pytorch/test_data/export/superres_epoch100-44c6958e.pth'\n",
    "batch_size = 1\n",
    "\n",
    "# Initialize model with the pretrained weights\n",
    "map_location = lambda storage, loc: storage\n",
    "if torch.cuda.is_available():\n",
    "    map_location = None\n",
    "torch_model.load_state_dict(model_zoo.load_url(model_url, map_location=map_location))\n",
    "\n",
    "# set the model to inference mode\n",
    "torch_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export model into ONNX format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_file_path = \"super_resolution.onnx\"\n",
    "\n",
    "# Input to the model\n",
    "torch_input = torch.randn(batch_size, 1, 224, 224, requires_grad=True)\n",
    "torch_out = torch_model(torch_input)\n",
    "\n",
    "# Export the model\n",
    "torch.onnx.export(\n",
    "    torch_model,               # model being run\n",
    "    torch_input,               # model input (or a tuple for multiple inputs)\n",
    "    onnx_file_path,            # where to save the model (can be a file or file-like object)\n",
    "    export_params=True,        # store the trained parameter weights inside the model file\n",
    "    opset_version=10,          # the ONNX version to export the model to\n",
    "    do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "    input_names = ['input'],   # the model's input names\n",
    "    output_names = ['output'], # the model's output names\n",
    "    dynamic_axes={'input' : {0 : 'batch_size'},    # variable lenght axes\n",
    "                'output' : {0 : 'batch_size'}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Model to BlobHub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model uploading code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from blobhub.blob import Blob, Revision\n",
    "from blobhub.presets.onnx import Onnx, Model        \n",
    "        \n",
    "# Find blob\n",
    "blob = Blob(org_id=ORG_ID, blob_id=BLOB_ID, api_key=API_KEY)\n",
    "revision = blob.revisions.latest()\n",
    "\n",
    "# Revision must be \"draft\" and \"ready\"\n",
    "assert revision[Revision.FIELD_PHASE] == Revision.PHASE_DRAFT\n",
    "assert revision[Revision.FIELD_STATUS] == Revision.STATUS_READY\n",
    "\n",
    "# Initialize preset\n",
    "onnx = Onnx(revision=revision)\n",
    "\n",
    "# Upload the model to the revision\n",
    "initial_model = Model.from_local_file(onnx=onnx, path=onnx_file_path)\n",
    "success = onnx.upload(model=initial_model)\n",
    "assert True == success"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Model from BlobHub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model downloading code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from blobhub.blob import Blob, Revision\n",
    "from blobhub.presets.onnx import Onnx, Model       \n",
    "\n",
    "# Find blob\n",
    "blob = Blob(org_id=ORG_ID, blob_id=BLOB_ID, api_key=API_KEY)\n",
    "revision = blob.revisions.latest()\n",
    "\n",
    "# Initialize preset\n",
    "onnx = Onnx(revision=revision)\n",
    "\n",
    "# Download and save the model\n",
    "downloaded_model = onnx.download()\n",
    "assert None != downloaded_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloaded model is stored locally and is accessible under:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloaded_model.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Model Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check model correctness (ONNX model consistency check):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "\n",
    "onnx_model = onnx.load(downloaded_model.path)\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize ONNX runtime:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "\n",
    "ort_session = onnxruntime.InferenceSession(downloaded_model.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load input image and convert to tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Load the image \n",
    "input_image_url = \"https://static.blobhub.io/revisions/01cba831-d693-4adf-b0c8-ded7e553c019/bridge_224x224.jpg\"\n",
    "response = requests.get(input_image_url)\n",
    "input_image_original = Image.open(BytesIO(response.content))\n",
    "\n",
    "# Resize image to match model input dimensions\n",
    "resize = transforms.Resize([224, 224])\n",
    "input_image = resize(input_image_original)\n",
    "\n",
    "# Convert image into YCbCr color space \n",
    "img_ycbcr = input_image.convert(\"YCbCr\")\n",
    "img_in_y, img_in_cb, img_in_cr = img_ycbcr.split()\n",
    "\n",
    "# Create tensor based on luma signal\n",
    "to_tensor = transforms.ToTensor()\n",
    "img_in_y = to_tensor(img_in_y)\n",
    "img_in_y.unsqueeze_(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(img_in_y)}\n",
    "ort_outs = ort_session.run(None, ort_inputs)\n",
    "img_out_y_out = ort_outs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct image object based on the model output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Convert model output into an image (just Y channel)\n",
    "img_out_y = Image.fromarray(np.uint8((img_out_y_out[0] * 255.0).clip(0, 255)[0]), mode='L')\n",
    "\n",
    "# Reconstruct the image by using model-resized luma channel and maually-resized chroma components\n",
    "output_image = Image.merge(\n",
    "    \"YCbCr\", [\n",
    "        img_out_y,\n",
    "        img_in_cb.resize(img_out_y.size, Image.BICUBIC),\n",
    "        img_in_cr.resize(img_out_y.size, Image.BICUBIC),\n",
    "    ]).convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resized image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
