{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ONNX Tutorial - BiDAF Model - Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tutorial demonstrates loading trained model weights from BlobHub and using for inference. \n",
    "\n",
    "Bi-Directional Attention Flow for Machine Comprehension model is used in this tutorial \n",
    "(extractive question answering model).\n",
    "\n",
    "References: \n",
    "- BlobHub model  \n",
    "  https://blobhub.io/onnx-text-models/bi-att-flow\n",
    "- Model details  \n",
    "  https://allenai.github.io/bi-att-flow/  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [Install Dependencies](#Install-Dependencies)\n",
    "- [Download Model from BlobHub](#Download-Model-from-BlobHub)\n",
    "- [Model Inference](#Model-Inference)\n",
    "- [Example Questions](#Example-Questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following packages are required for this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install blobhub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Model from BlobHub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This snippet downloads model from public BlobHub blob. Blob reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORG_ID = \"onnx-text-models\"\n",
    "BLOB_ID = \"bi-att-flow\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model downloading code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from blobhub.blob import Blob, Revision\n",
    "from blobhub.presets.onnx import Onnx, Model       \n",
    "\n",
    "# Find blob\n",
    "blob = Blob(org_id=ORG_ID, blob_id=BLOB_ID)\n",
    "revision = blob.revisions.latest()\n",
    "\n",
    "# Initialize preset\n",
    "onnx = Onnx(revision=revision)\n",
    "\n",
    "# Download and save the model\n",
    "downloaded_model = onnx.download()\n",
    "assert None != downloaded_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloaded model is stored locally and is accessible under:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloaded_model.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check model correctness (ONNX model consistency check):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "\n",
    "onnx_model = onnx.load(downloaded_model.path)\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize ONNX runtime:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "\n",
    "ort_session = onnxruntime.InferenceSession(downloaded_model.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference helpers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "from nltk import word_tokenize\n",
    "\n",
    "def preprocess(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    # split into lower-case word tokens, in numpy array with shape of (seq, 1)\n",
    "    words = np.asarray([w.lower() for w in tokens]).reshape(-1, 1)\n",
    "    # split words into chars, in numpy array with shape of (seq, 1, 1, 16)\n",
    "    chars = [[c for c in t][:16] for t in tokens]\n",
    "    chars = [cs+['']*(16-len(cs)) for cs in chars]\n",
    "    chars = np.asarray(chars).reshape(-1, 1, 1, 16)\n",
    "    return words, chars\n",
    "\n",
    "def infer(ort_session, context, query):\n",
    "    # Prepare input\n",
    "    cw, cc = preprocess(context)\n",
    "    qw, qc = preprocess(query)\n",
    "    \n",
    "    # Run inference\n",
    "    ort_inputs = {\n",
    "        ort_session.get_inputs()[0].name: cw,\n",
    "        ort_session.get_inputs()[1].name: cc,\n",
    "        ort_session.get_inputs()[2].name: qw,\n",
    "        ort_session.get_inputs()[3].name: qc\n",
    "    }\n",
    "    ort_outs = ort_session.run(None, ort_inputs)    \n",
    "    \n",
    "    # Parse output\n",
    "    start = ort_outs[0].item()\n",
    "    end = ort_outs[1].item()\n",
    "    answer = [w.encode() for w in cw[start:end+1].reshape(-1)]\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer(\n",
    "    ort_session=ort_session, \n",
    "    context=\"A quick brown fox jumps over the lazy dog.\", \n",
    "    query=\"What color is the fox?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer(\n",
    "    ort_session=ort_session, \n",
    "    context=\"The tokenized words are in lower case, while chars are not.\", \n",
    "    query=\"What is not tokenized?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer(\n",
    "    ort_session=ort_session, \n",
    "    context=\n",
    "        \"A black hole is a region of spacetime where gravity is so strong that nothing\"\n",
    "        \" — no particles or even electromagnetic radiation such as light — can escape from it.\", \n",
    "    query=\"What is a black hole?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
